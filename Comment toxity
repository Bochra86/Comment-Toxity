import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
import re

nltk.download('stopwords')

# Load dataset (Jigsaw dataset for example)
data = pd.read_csv('train.csv')

# Preprocess text
STOPWORDS = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r'[^\w\s]', '', text.lower())  # Remove punctuation and lower text
    text = re.sub(r'\d+', '', text)  # Remove digits
    return ' '.join([word for word in text.split() if word not in STOPWORDS])

data['comment_text'] = data['comment_text'].apply(preprocess_text)

# Labels (binary classification: toxic or non-toxic)
labels = data['toxic']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data['comment_text'], labels, test_size=0.2, random_state=42)

# Tokenization and padding
MAX_NB_WORDS = 50000  # Maximum number of words to keep in the tokenizer
MAX_SEQUENCE_LENGTH = 250  # Maximum sequence length for padding
EMBEDDING_DIM = 100  # Embedding dimensions

tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)
tokenizer.fit_on_texts(X_train)
word_index = tokenizer.word_index
print(f'Found {len(word_index)} unique tokens.')

X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

X_train_padded = pad_sequences(X_train_sequences, maxlen=MAX_SEQUENCE_LENGTH)
X_test_padded = pad_sequences(X_test_sequences, maxlen=MAX_SEQUENCE_LENGTH)

# Build the model
model = Sequential()
model.add(Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))
model.add(SpatialDropout1D(0.2))  # Prevent overfitting
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))  # LSTM layer
model.add(Dense(1, activation='sigmoid'))  # Binary classification

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_padded, y_train, batch_size=64, epochs=5, validation_data=(X_test_padded, y_test), verbose=2)

# Evaluate the model
loss, accuracy = model.evaluate(X_test_padded, y_test, verbose=2)
print(f'Test Accuracy: {accuracy * 100:.2f}%')
